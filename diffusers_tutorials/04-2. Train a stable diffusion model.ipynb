{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337ad78b-84a4-4c90-8a20-0c8079a590eb",
   "metadata": {},
   "source": [
    "# **Text-to-image**\n",
    "\n",
    "    - 작성일 : 24.07.30  \n",
    "    - 작성자 : 유소영  \n",
    "    - 출처 : \n",
    "        https://huggingface.co/docs/diffusers/training/text2image\n",
    "        https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b030659-58cb-4ae9-9ab2-9bd28c1bbd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import accelerate\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.state import AcceleratorState\n",
    "from accelerate.utils import ProjectConfiguration, set_seed\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "from packaging import version\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from transformers.utils import ContextManagers\n",
    "\n",
    "import diffusers\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, StableDiffusionPipeline, UNet2DConditionModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.training_utils import EMAModel, compute_snr\n",
    "from diffusers.utils import check_min_version, deprecate, is_wandb_available, make_image_grid\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "\n",
    "if is_wandb_available():\n",
    "    import wandb\n",
    "\n",
    "\n",
    "# Will error if the minimal version of diffusers is not installed. Remove at your own risks.\n",
    "check_min_version(\"0.30.0.dev0\") #이 함수는 현재 설치된 Diffusers 라이브러리의 버전이 최소 요구 버전(\"0.30.0.dev0\") 이상인지 확인합니다.\n",
    "\n",
    "logger = get_logger(__name__, log_level=\"INFO\") # 현재 실행 중인 모듈 전체, 로그 레벨은 \"INFO\"로 설정되어 있어, 정보성 메시지부터 에러까지 모두 기록됩니다.\n",
    "\n",
    "DATASET_NAME_MAPPING = { \n",
    "    \"soyng/wheel-web\": (\"image\", \"text\"), #  데이터셋 이름과 해당 데이터셋의 컬럼 구조를 매핑하는 딕셔너리\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98fe5d68-e6b0-4658-9230-a44514e8173f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c82645ca4a94ac78e23b07f107982c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"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\"\n",
    "notebook_login()\n",
    "\n",
    "# setting > Access Tokens > (create new token) write > invalidate and refresh button "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28af86b5-a9b5-43a7-9494-73dd29fd3a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_card(args, repo_id: str, images=None, repo_folder=None, ):\n",
    "    yaml = f\"\"\"\n",
    "            ---\n",
    "            license: creativeml-openrail-m\n",
    "            base_model: {args.pretrained_model_name_or_path}\n",
    "            datasets:\n",
    "            - {args.dataset_name}\n",
    "            tags:\n",
    "            - stable-diffusion\n",
    "            - stable-diffusion-diffusers\n",
    "            - text-to-image\n",
    "            - diffusers\n",
    "            inference: true\n",
    "            ---\n",
    "                \"\"\"\n",
    "    model_card = f\"\"\"\n",
    "            # Text-to-image finetuning - {repo_id}\n",
    "            \n",
    "            This pipeline was finetuned from **{args.pretrained_model_name_or_path}** on the **{args.dataset_name}** dataset. Below are some example images generated with the finetuned pipeline using the following prompts: {args.validation_prompts}: \\n\n",
    "            {img_str}\n",
    "            \n",
    "            ## Pipeline usage\n",
    "            \n",
    "            You can use the pipeline like so:\n",
    "            \n",
    "            ```python\n",
    "            from diffusers import DiffusionPipeline\n",
    "            import torch\n",
    "            \n",
    "            pipeline = DiffusionPipeline.from_pretrained(\"{repo_id}\", torch_dtype=torch.float16)\n",
    "            prompt = \"{args.validation_prompts[0]}\"\n",
    "            image = pipeline(prompt).images[0]\n",
    "            image.save(\"my_image.png\")\n",
    "            ```\n",
    "\n",
    "            ## Training info\n",
    "            \n",
    "            These are the key hyperparameters used during training:\n",
    "            \n",
    "            * Epochs: {args.num_train_epochs}\n",
    "            * Learning rate: {args.learning_rate}\n",
    "            * Batch size: {args.train_batch_size}\n",
    "            * Gradient accumulation steps: {args.gradient_accumulation_steps}\n",
    "            * Image resolution: {args.resolution}\n",
    "            * Mixed-precision: {args.mixed_precision}\n",
    "            \n",
    "            \"\"\"\n",
    "    wandb_info = \"\"\n",
    "    if is_wandb_available():\n",
    "        wandb_run_url = None\n",
    "        if wandb.run is not None:\n",
    "            wandb_run_url = wandb.run.url\n",
    "\n",
    "    if wandb_run_url is not None:\n",
    "        wandb_info = f\"\"\"\n",
    "                      More information on all the CLI arguments and the environment are available on your [`wandb` run page]({wandb_run_url}).\n",
    "                     \"\"\"\n",
    "\n",
    "    model_card += wandb_info\n",
    "\n",
    "    with open(os.path.join(repo_folder, \"README.md\"), \"w\") as f:\n",
    "        f.write(yaml + model_card)\n",
    "\n",
    "\n",
    "def log_validation(vae, text_encoder, tokenizer, unet, args, accelerator, weight_dtype, epoch):\n",
    "    logger.info(\"검증 실행 중... \")\n",
    "    \n",
    "    # Stable Diffusion 파이프라인 생성\n",
    "    pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "        args.pretrained_model_name_or_path,\n",
    "        vae=accelerator.unwrap_model(vae),\n",
    "        text_encoder=accelerator.unwrap_model(text_encoder),\n",
    "        tokenizer=tokenizer,\n",
    "        unet=accelerator.unwrap_model(unet),\n",
    "        safety_checker=None,\n",
    "        revision=args.revision,\n",
    "        torch_dtype=weight_dtype,\n",
    "    )\n",
    "    \n",
    "    # 파이프라인을 현재 디바이스로 이동\n",
    "    pipeline = pipeline.to(accelerator.device)\n",
    "    \n",
    "    # 진행 바 비활성화\n",
    "    pipeline.set_progress_bar_config(disable=True)\n",
    "    \n",
    "    # xformers 메모리 효율적 어텐션 활성화 (옵션)\n",
    "    if args.enable_xformers_memory_efficient_attention:\n",
    "        pipeline.enable_xformers_memory_efficient_attention()\n",
    "    \n",
    "    # 랜덤 시드 설정\n",
    "    if args.seed is None:\n",
    "        generator = None\n",
    "    else:\n",
    "        generator = torch.Generator(device=accelerator.device).manual_seed(args.seed)\n",
    "    \n",
    "    images = []\n",
    "    # 각 검증 프롬프트에 대해 이미지 생성\n",
    "    for i in range(4):\n",
    "        with torch.autocast(\"cuda\"):\n",
    "            image = pipeline(args.validation_prompts, num_inference_steps=20, generator=generator).images[0]\n",
    "        images.append(image)\n",
    "    \n",
    "    # 트래커에 이미지 로깅\n",
    "    for tracker in accelerator.trackers:\n",
    "        if tracker.name == \"tensorboard\":\n",
    "            # TensorBoard에 이미지 로깅\n",
    "            np_images = np.stack([np.asarray(img) for img in images])\n",
    "            tracker.writer.add_images(\"validation\", np_images, epoch, dataformats=\"NHWC\")\n",
    "        elif tracker.name == \"wandb\":\n",
    "            # Weights & Biases에 이미지 로깅\n",
    "            tracker.log(\n",
    "                {\n",
    "                    \"validation\": [\n",
    "                        wandb.Image(image, caption=f\"{i}: {args.validation_prompts}\")\n",
    "                        for i, image in enumerate(images)\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            logger.warn(f\"{tracker.name}에 대한 이미지 로깅이 구현되지 않았습니다\")\n",
    "    \n",
    "    # 메모리 정리\n",
    "    del pipeline\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d01dd710-fd43-49e7-b0ae-0c6b1470ce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    input_perturbation: float = 0\n",
    "    pretrained_model_name_or_path: str = 'CompVis/stable-diffusion-v1-4'  # 사전 훈련된 모델 경로\n",
    "    revision: Optional[str] = None  # 모델 리비전\n",
    "    dataset_name: Optional[str] =  None # 데이터셋 이름\n",
    "    dataset_config_name: Optional[str] = None  # 데이터셋 설정\n",
    "    train_data_dir: Optional[str] =  '../data'  # 훈련 데이터 디렉토리\n",
    "    image_column: str = \"image\"\n",
    "    caption_column: str = \"text\"\n",
    "    max_train_samples: Optional[int] = None  # 최대 훈련 샘플 수\n",
    "    validation_prompts: Optional[List[str]] = 'High-performance car wheel rim, detailed 3D rendering'  # 검증 프롬프트\n",
    "    output_dir: str = 'sd-model-finetuned'\n",
    "    cache_dir: Optional[str] = None  # 캐시 디렉토리\n",
    "    seed: Optional[int] = None  # 랜덤 시드\n",
    "    resolution: int = 512\n",
    "    center_crop: bool = False\n",
    "    random_flip: bool = False\n",
    "    train_batch_size: int = 32\n",
    "    num_train_epochs: int = 60\n",
    "    max_train_steps: Optional[int] = None  # 최대 훈련 스텝 수\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    gradient_checkpointing: bool = False\n",
    "    learning_rate: float = 1e-5\n",
    "    scale_lr: bool = False\n",
    "    lr_scheduler: str = \"constant\"\n",
    "    lr_warmup_steps: int = 500\n",
    "    snr_gamma: Optional[float] = None  # SNR 가중치 감마\n",
    "    use_8bit_adam: bool = False\n",
    "    allow_tf32: bool = False\n",
    "    use_ema: bool = False\n",
    "    non_ema_revision: Optional[str] = None  # 비 EMA 모델 리비전\n",
    "    dataloader_num_workers: int = 0\n",
    "    adam_beta1: float = 0.9\n",
    "    adam_beta2: float = 0.999\n",
    "    adam_weight_decay: float = 1e-2\n",
    "    adam_epsilon: float = 1e-08\n",
    "    max_grad_norm: float = 1.0\n",
    "    push_to_hub: bool = True\n",
    "    hub_token: Optional[str] = None  # 모델 허브 토큰\n",
    "    prediction_type: Optional[str] = None  # 예측 타입\n",
    "    hub_model_id: Optional[str] = 'soyng/photorealistic-wheel-v1-0' # 모델 허브 ID\n",
    "    logging_dir: str = 'logs'\n",
    "    mixed_precision: Optional[str] = None  # 혼합 정밀도 설정\n",
    "    report_to: str = 'wandb'\n",
    "    local_rank: int = -1\n",
    "    checkpointing_steps: int = 500\n",
    "    checkpoints_total_limit: Optional[int] = None  # 최대 체크포인트 수\n",
    "    resume_from_checkpoint: Optional[str] = 'checkpoint-8500'  # 체크포인트에서 재개\n",
    "    enable_xformers_memory_efficient_attention: bool = False\n",
    "    noise_offset: float = 0\n",
    "    validation_epochs: int = 5\n",
    "    tracker_project_name: str = 'CompVis_stable-diffusion-v1-4-fine-tune'\n",
    "    \n",
    "# 설정 인스턴스 생성\n",
    "config = TrainingConfig()\n",
    "\n",
    "# 환경 변수에서 LOCAL_RANK 가져오기\n",
    "import os\n",
    "env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
    "if env_local_rank != -1 and env_local_rank != config.local_rank:\n",
    "    config.local_rank = env_local_rank\n",
    "\n",
    "# Sanity checks\n",
    "if config.dataset_name is None and config.train_data_dir is None:\n",
    "    raise ValueError(\"Need either a dataset name or a training folder.\")\n",
    "\n",
    "# default to using the same revision for the non-ema model if not specified\n",
    "if config.non_ema_revision is None:\n",
    "    config.non_ema_revision = config.revision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f42625-170c-4f52-945e-338190b5f49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "08/01/2024 02:51:06 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "{'clip_sample_range', 'prediction_type', 'rescale_betas_zero_snr', 'sample_max_value', 'thresholding', 'timestep_spacing', 'variance_type', 'dynamic_thresholding_ratio'} was not found in config. Values will be initialized to default values.\n",
      "{'use_quant_conv', 'shift_factor', 'use_post_quant_conv', 'norm_num_groups', 'latents_mean', 'latents_std', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
      "{'only_cross_attention', 'conv_in_kernel', 'transformer_layers_per_block', 'conv_out_kernel', 'encoder_hid_dim', 'time_embedding_dim', 'dropout', 'time_cond_proj_dim', 'dual_cross_attention', 'num_attention_heads', 'mid_block_type', 'use_linear_projection', 'addition_embed_type_num_heads', 'encoder_hid_dim_type', 'time_embedding_act_fn', 'timestep_post_act', 'resnet_skip_time_act', 'cross_attention_norm', 'addition_time_embed_dim', 'mid_block_only_cross_attention', 'attention_type', 'resnet_out_scale_factor', 'reverse_transformer_layers_per_block', 'class_embed_type', 'class_embeddings_concat', 'addition_embed_type', 'time_embedding_type', 'projection_class_embeddings_input_dim', 'num_class_embeds', 'resnet_time_scale_shift', 'upcast_attention'} was not found in config. Values will be initialized to default values.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5007841f930a44f0b6dfbd99cb3c0777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/4967 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msoyoung9306\u001b[0m (\u001b[33msoyoung9306-slack\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/opt/mnt/hugging_face/tutorials/wandb/run-20240801_025117-27944xzb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/soyoung9306-slack/CompVis_stable-diffusion-v1-4-fine-tune/runs/27944xzb' target=\"_blank\">still-frog-2</a></strong> to <a href='https://wandb.ai/soyoung9306-slack/CompVis_stable-diffusion-v1-4-fine-tune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/soyoung9306-slack/CompVis_stable-diffusion-v1-4-fine-tune' target=\"_blank\">https://wandb.ai/soyoung9306-slack/CompVis_stable-diffusion-v1-4-fine-tune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/soyoung9306-slack/CompVis_stable-diffusion-v1-4-fine-tune/runs/27944xzb' target=\"_blank\">https://wandb.ai/soyoung9306-slack/CompVis_stable-diffusion-v1-4-fine-tune/runs/27944xzb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/01/2024 02:51:18 - INFO - __main__ - ***** 훈련 시작 *****\n",
      "08/01/2024 02:51:18 - INFO - __main__ -   총 샘플 수 = 4962\n",
      "08/01/2024 02:51:18 - INFO - __main__ -   에포크 수 = 60\n",
      "08/01/2024 02:51:18 - INFO - __main__ -   디바이스당 배치 크기 = 32\n",
      "08/01/2024 02:51:18 - INFO - __main__ -   총 훈련 배치 크기 (병렬, 분산 & 누적) = 32\n",
      "08/01/2024 02:51:18 - INFO - __main__ -   그래디언트 누적 단계 = 1\n",
      "08/01/2024 02:51:18 - INFO - __main__ -   총 최적화 단계 = 9360\n",
      "08/01/2024 02:51:18 - INFO - accelerate.accelerator - Loading states from sd-model-finetuned/checkpoint-8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "체크포인트 checkpoint-8500에서 재개합니다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/01/2024 02:51:19 - INFO - accelerate.checkpointing - All model weights loaded successfully\n",
      "08/01/2024 02:51:23 - INFO - accelerate.checkpointing - All optimizer states loaded successfully\n",
      "08/01/2024 02:51:23 - INFO - accelerate.checkpointing - All scheduler states loaded successfully\n",
      "08/01/2024 02:51:23 - INFO - accelerate.checkpointing - All dataloader sampler states loaded successfully\n",
      "08/01/2024 02:51:23 - INFO - accelerate.checkpointing - All random states loaded successfully\n",
      "08/01/2024 02:51:23 - INFO - accelerate.accelerator - Loading in 0 custom states\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3080577d62f4d5abf9717dd30ae72d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:  91%|######### | 8500/9360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/01/2024 03:14:18 - INFO - __main__ - 검증 실행 중... \n",
      "{'requires_safety_checker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a11dbb678130462fb411c4104dd1e6ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
      "{'timestep_spacing', 'prediction_type'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
      "08/01/2024 03:28:12 - INFO - accelerate.accelerator - Saving current state to sd-model-finetuned/checkpoint-9000\n",
      "Configuration saved in sd-model-finetuned/checkpoint-9000/unet/config.json\n",
      "Model weights saved in sd-model-finetuned/checkpoint-9000/unet/diffusion_pytorch_model.safetensors\n",
      "08/01/2024 03:28:26 - INFO - accelerate.checkpointing - Optimizer state saved in sd-model-finetuned/checkpoint-9000/optimizer.bin\n",
      "08/01/2024 03:28:26 - INFO - accelerate.checkpointing - Scheduler state saved in sd-model-finetuned/checkpoint-9000/scheduler.bin\n",
      "08/01/2024 03:28:26 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in sd-model-finetuned/checkpoint-9000/sampler.bin\n",
      "08/01/2024 03:28:26 - INFO - accelerate.checkpointing - Random states saved in sd-model-finetuned/checkpoint-9000/random_states_0.pkl\n",
      "08/01/2024 03:28:26 - INFO - __main__ - 체크포인트 저장 위치: sd-model-finetuned/checkpoint-9000\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    args = TrainingConfig()\n",
    "\n",
    "    # non_ema_revision이 None이 아닌 경우 경고 메시지 출력\n",
    "    if args.non_ema_revision is not None:\n",
    "        deprecate(\n",
    "            \"non_ema_revision!=None\",\n",
    "            \"0.15.0\",\n",
    "            message=(\n",
    "                \"Hub에서 'non_ema' 가중치를 리비전 브랜치에서 다운로드하는 것은 더 이상 사용되지 않습니다. \"\n",
    "                \"`--variant=non_ema`를 대신 사용해주세요.\"\n",
    "            ),\n",
    "        )\n",
    "    \n",
    "    # 로깅 디렉토리 설정\n",
    "    logging_dir = os.path.join(args.output_dir, args.logging_dir)\n",
    "\n",
    "    # Accelerator 프로젝트 설정\n",
    "    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)\n",
    "\n",
    "    # Accelerator 초기화\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        mixed_precision=args.mixed_precision,\n",
    "        log_with=args.report_to,\n",
    "        project_config=accelerator_project_config,\n",
    "    )\n",
    "\n",
    "    # 로깅 설정\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "    logger.info(accelerator.state, main_process_only=False)\n",
    "    \n",
    "    # 메인 프로세스에서만 로깅 레벨 설정\n",
    "    if accelerator.is_local_main_process:\n",
    "        datasets.utils.logging.set_verbosity_warning()\n",
    "        transformers.utils.logging.set_verbosity_warning()\n",
    "        diffusers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "        diffusers.utils.logging.set_verbosity_error()\n",
    "\n",
    "    # 시드 설정\n",
    "    if args.seed is not None:\n",
    "        set_seed(args.seed)\n",
    "\n",
    "    # 출력 디렉토리 생성 및 Hub 관련 설정\n",
    "    if accelerator.is_main_process:\n",
    "        if args.output_dir is not None:\n",
    "            os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "        if args.push_to_hub:\n",
    "            repo_id = create_repo(\n",
    "                repo_id=args.hub_model_id or Path(args.output_dir).name, exist_ok=True, token=args.hub_token\n",
    "            ).repo_id\n",
    "\n",
    "    # 스케줄러, 토크나이저, 모델 로드\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\n",
    "        args.pretrained_model_name_or_path, subfolder=\"tokenizer\", revision=args.revision\n",
    "    )\n",
    "\n",
    "    # Deepspeed ZeRO 초기화 비활성화 컨텍스트 매니저 정의\n",
    "    def deepspeed_zero_init_disabled_context_manager():\n",
    "        deepspeed_plugin = AcceleratorState().deepspeed_plugin if accelerate.state.is_initialized() else None\n",
    "        if deepspeed_plugin is None:\n",
    "            return []\n",
    "        return [deepspeed_plugin.zero3_init_context_manager(enable=False)]\n",
    "\n",
    "    # 텍스트 인코더와 VAE 모델 로드\n",
    "    with ContextManagers(deepspeed_zero_init_disabled_context_manager()):\n",
    "        text_encoder = CLIPTextModel.from_pretrained(\n",
    "            args.pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=args.revision\n",
    "        )\n",
    "        vae = AutoencoderKL.from_pretrained(\n",
    "            args.pretrained_model_name_or_path, subfolder=\"vae\", revision=args.revision\n",
    "        )\n",
    "\n",
    "    # UNet 모델 로드\n",
    "    unet = UNet2DConditionModel.from_pretrained(\n",
    "        args.pretrained_model_name_or_path, subfolder=\"unet\", revision=args.non_ema_revision\n",
    "    )\n",
    "\n",
    "    # VAE와 텍스트 인코더를 고정하고 UNet을 훈련 가능하게 설정\n",
    "    vae.requires_grad_(False)\n",
    "    text_encoder.requires_grad_(False)\n",
    "    unet.train()\n",
    "\n",
    "    # UNet의 EMA 모델 생성 (옵션)\n",
    "    if args.use_ema:\n",
    "        ema_unet = UNet2DConditionModel.from_pretrained(\n",
    "            args.pretrained_model_name_or_path, subfolder=\"unet\", revision=args.revision\n",
    "        )\n",
    "        ema_unet = EMAModel(ema_unet.parameters(), model_cls=UNet2DConditionModel, model_config=ema_unet.config)\n",
    "\n",
    "    # xformers 메모리 효율적 어텐션 활성화 (옵션)\n",
    "    if args.enable_xformers_memory_efficient_attention:\n",
    "        if is_xformers_available():\n",
    "            import xformers\n",
    "            xformers_version = version.parse(xformers.__version__)\n",
    "            if xformers_version == version.parse(\"0.0.16\"):\n",
    "                logger.warn(\n",
    "                    \"xFormers 0.0.16은 일부 GPU에서 훈련에 사용할 수 없습니다. 훈련 중 문제가 발생하면 xFormers를 최소 0.0.17 버전으로 업데이트하세요.\"\n",
    "                )\n",
    "            unet.enable_xformers_memory_efficient_attention()\n",
    "        else:\n",
    "            raise ValueError(\"xformers를 사용할 수 없습니다. 올바르게 설치되었는지 확인하세요.\")\n",
    "\n",
    "    # 커스텀 저장 및 로딩 훅 정의\n",
    "    if version.parse(accelerate.__version__) >= version.parse(\"0.16.0\"):\n",
    "        def save_model_hook(models, weights, output_dir):\n",
    "            if accelerator.is_main_process:\n",
    "                if args.use_ema:\n",
    "                    ema_unet.save_pretrained(os.path.join(output_dir, \"unet_ema\"))\n",
    "\n",
    "                for i, model in enumerate(models):\n",
    "                    model.save_pretrained(os.path.join(output_dir, \"unet\"))\n",
    "                    weights.pop()\n",
    "\n",
    "        def load_model_hook(models, input_dir):\n",
    "            if args.use_ema:\n",
    "                load_model = EMAModel.from_pretrained(os.path.join(input_dir, \"unet_ema\"), UNet2DConditionModel)\n",
    "                ema_unet.load_state_dict(load_model.state_dict())\n",
    "                ema_unet.to(accelerator.device)\n",
    "                del load_model\n",
    "\n",
    "            for i in range(len(models)):\n",
    "                model = models.pop()\n",
    "                load_model = UNet2DConditionModel.from_pretrained(input_dir, subfolder=\"unet\")\n",
    "                model.register_to_config(**load_model.config)\n",
    "                model.load_state_dict(load_model.state_dict())\n",
    "                del load_model\n",
    "\n",
    "        accelerator.register_save_state_pre_hook(save_model_hook)\n",
    "        accelerator.register_load_state_pre_hook(load_model_hook)\n",
    "\n",
    "    # 그래디언트 체크포인팅 활성화 (옵션)\n",
    "    if args.gradient_checkpointing:\n",
    "        unet.enable_gradient_checkpointing()\n",
    "\n",
    "    # TF32 정밀도 허용 (옵션)\n",
    "    if args.allow_tf32:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "    # 학습률 스케일링 (옵션)\n",
    "    if args.scale_lr:\n",
    "        args.learning_rate = (\n",
    "            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes\n",
    "        )\n",
    "\n",
    "    # 8비트 Adam 옵티마이저 사용 (옵션)\n",
    "    if args.use_8bit_adam:\n",
    "        try:\n",
    "            import bitsandbytes as bnb\n",
    "        except ImportError:\n",
    "            raise ImportError(\"8비트 Adam을 사용하려면 bitsandbytes를 설치하세요. `pip install bitsandbytes`로 설치할 수 있습니다.\")\n",
    "        optimizer_cls = bnb.optim.AdamW8bit\n",
    "    else:\n",
    "        optimizer_cls = torch.optim.AdamW\n",
    "\n",
    "    # 옵티마이저 초기화\n",
    "    optimizer = optimizer_cls(\n",
    "        unet.parameters(),\n",
    "        lr=args.learning_rate,\n",
    "        betas=(args.adam_beta1, args.adam_beta2),\n",
    "        weight_decay=args.adam_weight_decay,\n",
    "        eps=args.adam_epsilon,\n",
    "    )\n",
    "\n",
    "    # 데이터셋 로드\n",
    "    if args.dataset_name is not None:\n",
    "        dataset = load_dataset(\n",
    "            args.dataset_name,\n",
    "            args.dataset_config_name,\n",
    "            cache_dir=args.cache_dir,\n",
    "            data_dir=args.train_data_dir,\n",
    "        )\n",
    "    else:\n",
    "        data_files = {}\n",
    "        if args.train_data_dir is not None:\n",
    "            data_files[\"train\"] = os.path.join(args.train_data_dir, \"**\")\n",
    "        dataset = load_dataset(\n",
    "            \"imagefolder\",\n",
    "            data_files=data_files,\n",
    "            cache_dir=args.cache_dir,\n",
    "        )\n",
    "\n",
    "    # 데이터셋 컬럼 이름 설정\n",
    "    column_names = dataset[\"train\"].column_names\n",
    "\n",
    "    dataset_columns = DATASET_NAME_MAPPING.get(args.dataset_name, None)\n",
    "    if args.image_column is None:\n",
    "        image_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\n",
    "    else:\n",
    "        image_column = args.image_column\n",
    "        if image_column not in column_names:\n",
    "            raise ValueError(f\"--image_column 값 '{args.image_column}'은 다음 중 하나여야 합니다: {', '.join(column_names)}\")\n",
    "    if args.caption_column is None:\n",
    "        caption_column = dataset_columns[1] if dataset_columns is not None else column_names[1]\n",
    "    else:\n",
    "        caption_column = args.caption_column\n",
    "        if caption_column not in column_names:\n",
    "            raise ValueError(f\"--caption_column 값 '{args.caption_column}'은 다음 중 하나여야 합니다: {', '.join(column_names)}\")\n",
    "\n",
    "    # 캡션 토큰화 함수 정의\n",
    "    def tokenize_captions(examples, is_train=True):\n",
    "        captions = []\n",
    "        for caption in examples[caption_column]:\n",
    "            if isinstance(caption, str):\n",
    "                captions.append(caption)\n",
    "            elif isinstance(caption, (list, np.ndarray)):\n",
    "                captions.append(random.choice(caption) if is_train else caption[0])\n",
    "            else:\n",
    "                raise ValueError(f\"캡션 컬럼 `{caption_column}`은 문자열 또는 문자열 리스트를 포함해야 합니다.\")\n",
    "        inputs = tokenizer(\n",
    "            captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        return inputs.input_ids\n",
    "\n",
    "    # 훈련 데이터 변환 정의\n",
    "    train_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(args.resolution) if args.center_crop else transforms.RandomCrop(args.resolution),\n",
    "            transforms.RandomHorizontalFlip() if args.random_flip else transforms.Lambda(lambda x: x),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 훈련 데이터 전처리 함수 정의\n",
    "    def preprocess_train(examples):\n",
    "        images = [image.convert(\"RGB\") for image in examples[image_column]]\n",
    "        examples[\"pixel_values\"] = [train_transforms(image) for image in images]\n",
    "        examples[\"input_ids\"] = tokenize_captions(examples)\n",
    "        return examples\n",
    "\n",
    "    # 데이터셋 전처리 및 샘플링\n",
    "    with accelerator.main_process_first():\n",
    "        if args.max_train_samples is not None:\n",
    "            dataset[\"train\"] = dataset[\"train\"].shuffle(seed=args.seed).select(range(args.max_train_samples))\n",
    "        train_dataset = dataset[\"train\"].with_transform(preprocess_train)\n",
    "\n",
    "    # 데이터 로더용 콜레이트 함수 정의\n",
    "    def collate_fn(examples):\n",
    "        pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "        input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
    "        return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n",
    "\n",
    "    # 데이터 로더 생성\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        batch_size=args.train_batch_size,\n",
    "        num_workers=args.dataloader_num_workers,\n",
    "    )\n",
    "\n",
    "    # 훈련 스텝 수 계산\n",
    "    overrode_max_train_steps = False\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "    if args.max_train_steps is None:\n",
    "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "        overrode_max_train_steps = True\n",
    "\n",
    "    # 학습률 스케줄러 초기화\n",
    "    lr_scheduler = get_scheduler(\n",
    "        args.lr_scheduler,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,\n",
    "        num_training_steps=args.max_train_steps * accelerator.num_processes,\n",
    "    )\n",
    "\n",
    "    # Accelerator를 사용하여 모델, 옵티마이저, 데이터 로더, 학습률 스케줄러 준비\n",
    "    unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        unet, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    # EMA 모델을 디바이스로 이동 (옵션)\n",
    "    if args.use_ema:\n",
    "        ema_unet.to(accelerator.device)\n",
    "\n",
    "    # 가중치 데이터 타입 설정\n",
    "    weight_dtype = torch.float32\n",
    "    if accelerator.mixed_precision == \"fp16\":\n",
    "        weight_dtype = torch.float16\n",
    "        args.mixed_precision = accelerator.mixed_precision\n",
    "    elif accelerator.mixed_precision == \"bf16\":\n",
    "        weight_dtype = torch.bfloat16\n",
    "        args.mixed_precision = accelerator.mixed_precision\n",
    "\n",
    "    # 텍스트 인코더와 VAE를 지정된 데이터 타입으로 변환\n",
    "    text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "    vae.to(accelerator.device, dtype=weight_dtype)\n",
    "\n",
    "    # 훈련 파라미터 재계산\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "    if overrode_max_train_steps:\n",
    "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "    # 트래커 초기화 (메인 프로세스에서만)\n",
    "    if accelerator.is_main_process:\n",
    "        tracker_config = dict(vars(args))\n",
    "        tracker_config.pop(\"validation_prompts\")\n",
    "        accelerator.init_trackers(args.tracker_project_name, tracker_config)\n",
    "\n",
    "    # 훈련 시작\n",
    "    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "\n",
    "    logger.info(\"***** 훈련 시작 *****\")\n",
    "    logger.info(f\"  총 샘플 수 = {len(train_dataset)}\")\n",
    "    logger.info(f\"  에포크 수 = {args.num_train_epochs}\")\n",
    "    logger.info(f\"  디바이스당 배치 크기 = {args.train_batch_size}\")\n",
    "    logger.info(f\"  총 훈련 배치 크기 (병렬, 분산 & 누적) = {total_batch_size}\")\n",
    "    logger.info(f\"  그래디언트 누적 단계 = {args.gradient_accumulation_steps}\")\n",
    "    logger.info(f\"  총 최적화 단계 = {args.max_train_steps}\")\n",
    "    \n",
    "    global_step = 0\n",
    "    first_epoch = 0\n",
    "\n",
    "    # 체크포인트에서 재개 (옵션)\n",
    "    if args.resume_from_checkpoint:\n",
    "        # 체크포인트 경로 설정\n",
    "        if args.resume_from_checkpoint != \"latest\":\n",
    "            path = os.path.basename(args.resume_from_checkpoint)\n",
    "        else:\n",
    "            # 가장 최근 체크포인트 찾기\n",
    "            dirs = os.listdir(args.output_dir)\n",
    "            dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n",
    "            dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n",
    "            path = dirs[-1] if len(dirs) > 0 else None\n",
    "\n",
    "        if path is None:\n",
    "            accelerator.print(f\"체크포인트 '{args.resume_from_checkpoint}'가 존재하지 않습니다. 새로운 훈련을 시작합니다.\")\n",
    "            args.resume_from_checkpoint = None\n",
    "            initial_global_step = 0\n",
    "        else:\n",
    "            accelerator.print(f\"체크포인트 {path}에서 재개합니다\")\n",
    "            accelerator.load_state(os.path.join(args.output_dir, path))\n",
    "            global_step = int(path.split(\"-\")[1])\n",
    "\n",
    "            initial_global_step = global_step\n",
    "            first_epoch = global_step // num_update_steps_per_epoch\n",
    "    else:\n",
    "        initial_global_step = 0\n",
    "\n",
    "    progress_bar = tqdm(\n",
    "        range(0, args.max_train_steps),\n",
    "        initial=initial_global_step,\n",
    "        desc=\"Steps\",\n",
    "        disable=not accelerator.is_local_main_process,\n",
    "    )\n",
    "\n",
    "    # 훈련 루프\n",
    "    for epoch in range(first_epoch, args.num_train_epochs):\n",
    "        train_loss = 0.0\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            with accelerator.accumulate(unet):\n",
    "                # 이미지를 잠재 공간으로 변환\n",
    "                latents = vae.encode(batch[\"pixel_values\"].to(weight_dtype)).latent_dist.sample()\n",
    "                latents = latents * vae.config.scaling_factor\n",
    "\n",
    "                # 노이즈 샘플링\n",
    "                noise = torch.randn_like(latents)\n",
    "                if args.noise_offset:\n",
    "                    # https://www.crosslabs.org//blog/diffusion-with-offset-noise\n",
    "                    noise += args.noise_offset * torch.randn(\n",
    "                        (latents.shape[0], latents.shape[1], 1, 1), device=latents.device\n",
    "                    )\n",
    "                if args.input_perturbation:\n",
    "                    new_noise = noise + args.input_perturbation * torch.randn_like(noise)\n",
    "                \n",
    "                bsz = latents.shape[0]\n",
    "                # 각 이미지에 대해 랜덤 타임스텝 샘플링\n",
    "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
    "                timesteps = timesteps.long()\n",
    "\n",
    "                # 노이즈 추가 (forward diffusion process)\n",
    "                if args.input_perturbation:\n",
    "                    noisy_latents = noise_scheduler.add_noise(latents, new_noise, timesteps)\n",
    "                else:\n",
    "                    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "                # 텍스트 임베딩 얻기\n",
    "                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
    "\n",
    "                # 예측 타입에 따른 타겟 설정\n",
    "                if args.prediction_type is not None:\n",
    "                    noise_scheduler.register_to_config(prediction_type=args.prediction_type)\n",
    "\n",
    "                if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "                    target = noise\n",
    "                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "                else:\n",
    "                    raise ValueError(f\"알 수 없는 예측 타입 {noise_scheduler.config.prediction_type}\")\n",
    "\n",
    "                # 노이즈 잔차 예측 및 손실 계산\n",
    "                model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "                if args.snr_gamma is None:\n",
    "                    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "                else:\n",
    "                    # SNR 가중치 계산 (https://arxiv.org/abs/2303.09556)\n",
    "                    snr = compute_snr(noise_scheduler, timesteps)\n",
    "                    if noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                        snr = snr + 1\n",
    "                    mse_loss_weights = (\n",
    "                        torch.stack([snr, args.snr_gamma * torch.ones_like(timesteps)], dim=1).min(dim=1)[0] / snr\n",
    "                    )\n",
    "\n",
    "                    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"none\")\n",
    "                    loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights\n",
    "                    loss = loss.mean()\n",
    "\n",
    "                # 손실 평균 계산\n",
    "                avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean()\n",
    "                train_loss += avg_loss.item() / args.gradient_accumulation_steps\n",
    "\n",
    "                # 역전파\n",
    "                accelerator.backward(loss)\n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(unet.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Accelerator가 최적화 단계를 수행했는지 확인\n",
    "            if accelerator.sync_gradients:\n",
    "                if args.use_ema:\n",
    "                    ema_unet.step(unet.parameters())\n",
    "                progress_bar.update(1)\n",
    "                global_step += 1\n",
    "                accelerator.log({\"train_loss\": train_loss}, step=global_step)\n",
    "                train_loss = 0.0\n",
    "\n",
    "                # 체크포인트 저장\n",
    "                if global_step % args.checkpointing_steps == 0:\n",
    "                    if accelerator.is_main_process:\n",
    "                        # 체크포인트 제한 확인\n",
    "                        if args.checkpoints_total_limit is not None:\n",
    "                            checkpoints = os.listdir(args.output_dir)\n",
    "                            checkpoints = [d for d in checkpoints if d.startswith(\"checkpoint\")]\n",
    "                            checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
    "\n",
    "                            # 체크포인트 제거\n",
    "                            if len(checkpoints) >= args.checkpoints_total_limit:\n",
    "                                num_to_remove = len(checkpoints) - args.checkpoints_total_limit + 1\n",
    "                                removing_checkpoints = checkpoints[0:num_to_remove]\n",
    "\n",
    "                                logger.info(\n",
    "                                    f\"{len(checkpoints)} 체크포인트가 이미 존재합니다. {len(removing_checkpoints)} 체크포인트를 제거합니다.\"\n",
    "                                )\n",
    "                                logger.info(f\"제거할 체크포인트: {', '.join(removing_checkpoints)}\")\n",
    "\n",
    "                                for removing_checkpoint in removing_checkpoints:\n",
    "                                    removing_checkpoint = os.path.join(args.output_dir, removing_checkpoint)\n",
    "                                    shutil.rmtree(removing_checkpoint)\n",
    "\n",
    "                        save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n",
    "                        accelerator.save_state(save_path)\n",
    "                        logger.info(f\"체크포인트 저장 위치: {save_path}\")\n",
    "\n",
    "            logs = {\"step_loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "\n",
    "            if global_step >= args.max_train_steps:\n",
    "                break\n",
    "\n",
    "        # 검증 (옵션)\n",
    "        if accelerator.is_main_process:\n",
    "            if args.validation_prompts is not None and epoch % args.validation_epochs == 0:\n",
    "                if args.use_ema:\n",
    "                    # EMA 매개변수로 임시 교체\n",
    "                    ema_unet.store(unet.parameters())\n",
    "                    ema_unet.copy_to(unet.parameters())\n",
    "                log_validation(\n",
    "                    vae,\n",
    "                    text_encoder,\n",
    "                    tokenizer,\n",
    "                    unet,\n",
    "                    args,\n",
    "                    accelerator,\n",
    "                    weight_dtype,\n",
    "                    global_step,\n",
    "                )\n",
    "                if args.use_ema:\n",
    "                    # 원래 UNet 매개변수로 복원\n",
    "                    ema_unet.restore(unet.parameters())\n",
    "\n",
    "    # 훈련된 모델을 사용하여 파이프라인 생성 및 저장\n",
    "    accelerator.wait_for_everyone()\n",
    "    if accelerator.is_main_process:\n",
    "        unet = accelerator.unwrap_model(unet)\n",
    "        if args.use_ema:\n",
    "            ema_unet.copy_to(unet.parameters())\n",
    "\n",
    "        pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "            args.pretrained_model_name_or_path,\n",
    "            text_encoder=text_encoder,\n",
    "            vae=vae,\n",
    "            unet=unet,\n",
    "            revision=args.revision,\n",
    "        )\n",
    "        pipeline.save_pretrained(args.output_dir)\n",
    "\n",
    "        # 최종 추론 실행\n",
    "        images = []\n",
    "        if args.validation_prompts is not None:\n",
    "            logger.info(\"생성된 이미지 수집을 위한 추론 실행 중...\")\n",
    "            pipeline = pipeline.to(accelerator.device)\n",
    "            pipeline.torch_dtype = weight_dtype\n",
    "            pipeline.set_progress_bar_config(disable=True)\n",
    "\n",
    "            if args.enable_xformers_memory_efficient_attention:\n",
    "                pipeline.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "            if args.seed is None:\n",
    "                generator = None\n",
    "                \n",
    "            else:\n",
    "                generator = torch.Generator(device=accelerator.device).manual_seed(args.seed)\n",
    "\n",
    "            for i in range(len(args.validation_prompts)):\n",
    "                with torch.autocast(\"cuda\"):\n",
    "                    image = pipeline(args.validation_prompts[i], num_inference_steps=20, generator=generator).images[0]\n",
    "                images.append(image)\n",
    "\n",
    "        if args.push_to_hub:\n",
    "            save_model_card(args, repo_id, images, repo_folder=args.output_dir)\n",
    "            upload_folder(\n",
    "                repo_id=repo_id,\n",
    "                folder_path=args.output_dir,\n",
    "                commit_message=\"훈련 종료\",\n",
    "                ignore_patterns=[\"step_*\", \"epoch_*\"],\n",
    "            )\n",
    "\n",
    "    accelerator.end_training()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a3c5b6-9b78-4ec5-9b49-0b74e409c001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/soyng_env/lib/python3.10/site-packages/huggingface_hub/hf_api.py:3664: UserWarning: Warnings while validating metadata in README.md:\n",
      "- empty or missing yaml metadata in repo card\n",
      "  warnings.warn(f\"Warnings while validating metadata in README.md:\\n{message}\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d302b87fb64acda4d0632b0db182b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args = TrainingConfig()\n",
    "if config.push_to_hub:\n",
    "    repo_id = create_repo(repo_id=config.hub_model_id or Path(config.output_dir).name\n",
    "                          , exist_ok=True).repo_id\n",
    "    #save_model_card(args, repo_id, None, repo_folder=args.output_dir)\n",
    "    upload_folder(\n",
    "        repo_id=repo_id,\n",
    "        folder_path=config.output_dir,\n",
    "        commit_message=f\"Epoch {config.num_train_epochs}\",\n",
    "        ignore_patterns=[\"epoch_*\"],)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86a2c08-b158-414d-94dd-45d5210fa2d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
