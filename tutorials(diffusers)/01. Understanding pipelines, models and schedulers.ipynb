{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c181c1-37af-418a-8b6c-9020854f5e02",
   "metadata": {},
   "source": [
    "# **Understanding pipelines, models and schedulers**\n",
    "\n",
    "- 작성일 : 24.07.30  \n",
    "- 작성자 : 유소영  \n",
    "- 출처 : https://huggingface.co/docs/diffusers/using-diffusers/write_own_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfe05ca-73c6-4500-be1c-2166014599ae",
   "metadata": {},
   "source": [
    "Diffusers는 사용자 친화적이고 유연한 도구로, 당신의 사용 사례에 맞는 확산 시스템을 구축하도록 설계되었습니다. \n",
    "\n",
    "이 도구의 핵심은 <u>모델과 스케줄러</u>입니다. DiffusionPipeline이 편의를 위해 이 구성 요소들을 함께 묶어주지만, 파이프라인을 해체하여 모델과 스케줄러를 별도로 사용해 새로운 확산 시스템을 만들 수도 있습니다.\n",
    "\n",
    "이 튜토리얼에서는 모델과 스케줄러를 사용하여 추론을 위한 확산 시스템을 조립하는 방법을 배웁니다. 기본 파이프라인부터 시작해 Stable Diffusion 파이프라인까지 진행합니다.\n",
    "\n",
    "기본 파이프라인 해체하기\n",
    "파이프라인은 모델을 추론에 사용하는 빠르고 쉬운 방법입니다. 이미지를 생성하는 데 단 4줄의 코드만 필요합니다:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11df14e1-62d4-4649-932e-f544c6027a4b",
   "metadata": {},
   "source": [
    "## Deconstruct a basic pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0d2e20-1f75-44f5-83e5-b11af66dfc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMPipeline\n",
    "\n",
    "ddpm = DDPMPipeline.from_pretrained(\"google/ddpm-cat-256\", use_safetensors=True).to(\"cuda\")\n",
    "image = ddpm(num_inference_steps=250).images[0]\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7f39c8-f601-4d62-bd87-12b003fe8c0f",
   "metadata": {},
   "source": [
    "이 과정은 매우 간단해 보이지만, 파이프라인이 어떻게 작동하는지 자세히 살펴보겠습니다.\n",
    "\n",
    "위 예시에서 파이프라인은 UNet2DModel 모델과 DDPMScheduler를 포함합니다. 파이프라인은 원하는 출력 크기의 랜덤 노이즈를 여러 번 모델에 통과시켜 이미지의 노이즈를 제거합니다.  \n",
    "각 timestep에서 모델은 '노이즈 잔차'를 예측하고, 스케줄러는 이를 사용해 덜 노이즈가 있는 이미지를 예측합니다. 파이프라인은 지정된 추론 단계 수에 도달할 때까지 이 과정을 반복합니다.\n",
    "모델과 스케줄러를 별도로 사용하여 파이프라인을 재현하기 위해, 자체적인 디노이징 프로세스를 작성해 보겠습니다.  \n",
    "\n",
    "**1. Load the model and scheduler:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2481d6b4-5103-47d2-9b37-0d07f94b5980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler, UNet2DModel\n",
    "\n",
    "scheduler = DDPMScheduler.from_pretrained(\"google/ddpm-cat-256\")\n",
    "model = UNet2DModel.from_pretrained(\"google/ddpm-cat-256\", use_safetensors=True).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd6605d-874a-423b-b19c-94e72cb2e5a4",
   "metadata": {},
   "source": [
    "**2. Set the number of timesteps to run the denoising process for:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cd67bd-59cb-4490-a1f4-7c21cd5cdb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.set_timesteps(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12481c17-5068-40cc-88b0-812b45208898",
   "metadata": {},
   "source": [
    "**3. 스케줄러 timesteps를 설정하면 균일하게 분포된 요소를 가진 텐서가 생성됩니다. 이 예시에서는 50개입니다. 각 요소는 모델이 이미지의 노이즈를 제거하는 timestep에 해당합니다. 나중에 디노이징 루프를 만들 때, 이 텐서를 반복하며 이미지의 노이즈를 제거하게 됩니다:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d291cabc-7f5c-435c-9728-604db3302e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scheduler.timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7eb6d2-3368-45cf-8435-ba3490fc1fa5",
   "metadata": {},
   "source": [
    "**4. 원하는 출력과 같은 형태의 랜덤 노이즈를 생성합니다:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea6f31c-2227-42a9-a68b-0577d282c3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "sample_size = model.config.sample_size\n",
    "noise = torch.randn((1, 3, sample_size, sample_size), device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7259826f-b42e-4480-929d-298eef4b5aff",
   "metadata": {},
   "source": [
    "**5. 이제 timesteps를 반복하는 루프를 작성합니다.**  각 timestep에서 모델은 UNet2DModel.forward() 패스를 수행하고 노이즈 잔차를 반환합니다. 스케줄러의 step() 메소드는 노이즈 잔차, timestep, 그리고 입력을 받아 이전 timestep의 이미지를 예측합니다. 이 출력은 디노이징 루프에서 모델의 다음 입력이 되며, `timesteps` 배열의 끝에 도달할 때까지 이 과정이 반복됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d90af1-5dad-4119-ad84-3d1980bec47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = noise\n",
    "\n",
    "for t in scheduler.timesteps:\n",
    "    with torch.no_grad():\n",
    "        noisy_residual = model(input, t).sample\n",
    "    previous_noisy_sample = scheduler.step(noisy_residual, t, input).prev_sample\n",
    "    input = previous_noisy_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7ae0f5-eb60-47a1-b073-a9cae7e1c6dc",
   "metadata": {},
   "source": [
    "**6. The last step is to convert the denoised output into an image:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994fda24-5010-46ff-9d44-255aedc8c518",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "image = (input / 2 + 0.5).clamp(0, 1).squeeze()\n",
    "image = (image.permute(1, 2, 0) * 255).round().to(torch.uint8).cpu().numpy()\n",
    "image = Image.fromarray(image)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cf5a23-c517-4c66-ab39-0031c80f9faf",
   "metadata": {},
   "source": [
    "## Deconstruct the Stable Diffusion pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa16006-7c82-47d0-a1e5-36c0c7eb354e",
   "metadata": {},
   "source": [
    "Stable Diffusion은 텍스트-이미지 '잠재 확산(Latent Diffusion)' 모델입니다. '잠재 확산' 모델이라고 불리는 이유는 다음과 같습니다:\n",
    "\n",
    "실제 픽셀 공간 대신 이미지의 저차원 표현을 다룹니다. 이로 인해 메모리 효율성이 높아집니다.\n",
    "\n",
    "Stable Diffusion 모델의 주요 구성 요소:\n",
    "\n",
    "- 인코더: 이미지를 더 작은 표현으로 압축합니다.\n",
    "- 디코더: 압축된 표현을 다시 이미지로 변환합니다.\n",
    "- 토크나이저와 인코더: 텍스트 임베딩을 생성합니다 (텍스트-이미지 모델에 필요).\n",
    "- UNet 모델: 이전 예제에서 본 것과 같은 역할을 합니다.\n",
    "- 스케줄러: 노이즈 제거 과정을 관리합니다.\n",
    "\n",
    "이는 UNet 모델만 포함하는 DDPM 파이프라인보다 훨씬 복잡합니다. Stable Diffusion 모델은 세 개의 별도 사전 훈련된 모델을 가지고 있어 구조가 더 복잡합니다. Stable Diffusion 파이프라인에 필요한 구성 요소들을 알았으니, from_pretrained() 메서드를 사용해 이 모든 구성 요소를 로드하겠습니다.  \n",
    "\n",
    "이들은 사전 훈련된 runwayml/stable-diffusion-v1-5 체크포인트에서 찾을 수 있으며, 각 구성 요소는 별도의 하위 폴더에 저장되어 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b872ad8b-490d-4a8a-81a6-0c2b5cd0dd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", use_safetensors=True)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\", subfolder=\"text_encoder\", use_safetensors=True\n",
    ")\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", use_safetensors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7864137-3158-4936-9d98-04dde41dfc9d",
   "metadata": {},
   "source": [
    "기본 PNDMScheduler 대신 UniPCMultistepScheduler로 교체하여 다른 스케줄러를 얼마나 쉽게 연결할 수 있는지 확인해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cb43e6-aa6f-47b7-ab7d-e774895594b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UniPCMultistepScheduler\n",
    "\n",
    "scheduler = UniPCMultistepScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad1bcc7-754a-463c-acde-7c3f51fc59cd",
   "metadata": {},
   "source": [
    "추론 속도를 높이기 위해, 스케줄러와 달리 훈련 가능한 가중치를 가진 모델들을 GPU로 이동시킵니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7193a31-59d9-437e-becf-d2a126a95469",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device = \"cuda\"\n",
    "vae.to(torch_device)\n",
    "text_encoder.to(torch_device)\n",
    "unet.to(torch_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceea16e-de73-479c-aa34-29dcfb95174d",
   "metadata": {},
   "source": [
    "**텍스트 임베딩 생성**  \n",
    "다음 단계는 텍스트를 토큰화하여 임베딩을 생성하는 것입니다. 이 텍스트는 UNet 모델을 조건화하고 확산 과정을 입력 프롬프트와 유사한 방향으로 유도하는 데 사용됩니다.\n",
    "\n",
    "💡 `guidance_scale` 매개변수는 이미지 생성 시 프롬프트에 얼마나 가중치를 줄지 결정합니다.\n",
    "\n",
    "다른 것을 생성하고 싶다면 원하는 프롬프트를 자유롭게 선택하세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afc527a-21b7-442e-8538-ffae8dab4ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"a photograph of an astronaut riding a horse\"]\n",
    "height = 512  # default height of Stable Diffusion\n",
    "width = 512  # default width of Stable Diffusion\n",
    "num_inference_steps = 100  # Number of denoising steps\n",
    "guidance_scale = 7.5  # Scale for classifier-free guidance\n",
    "generator = torch.manual_seed(0)  # Seed generator to create the initial latent noise\n",
    "batch_size = len(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0393bf5f-a7e8-48fb-b484-1baa63d719a2",
   "metadata": {},
   "source": [
    "Tokenize the text and generate the embeddings from the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15ca331-28e2-4a35-b53e-9f8c60ef509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = tokenizer(\n",
    "    prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a078d6e3-5ff6-4bc7-a1c9-b162782f0a55",
   "metadata": {},
   "source": [
    "또한 'unconditional text embeddings'을 생성해야 합니다. 이는 패딩 토큰에 대한 임베딩입니다. 이들은 conditional text_embeddings와 같은 형태의(batch_size와 seq_length)를 가져야 합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037ebd84-6748-4c35-bb78-27d2537d484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = text_input.input_ids.shape[-1]\n",
    "uncond_input = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
    "uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ef15dc-6c10-4e8c-b377-bf629e7d6618",
   "metadata": {},
   "source": [
    "Let’s concatenate the conditional and unconditional embeddings into a batch to avoid doing two forward passes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9ce36b-d3a5-469d-bfef-a97f0f823c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccc3e4e-a7a2-42f9-bc41-1fe453cf6f0e",
   "metadata": {},
   "source": [
    "**랜덤 노이즈 생성**  \n",
    "다음으로, 확산 과정의 시작점으로 사용할 초기 랜덤 노이즈를 생성합니다. 이것은 이미지의 잠재 표현이며, 점진적으로 노이즈가 제거될 것입니다. 이 시점에서 `latent` 이미지는 최종 이미지 크기보다 작습니다. 하지만 이는 문제가 되지 않습니다. 왜냐하면 모델이 나중에 이를 최종적인 512x512 이미지 크기로 변환할 것이기 때문입니다.  \n",
    "💡 높이와 너비를 8로 나누는 이유는 vae 모델이 3개의 다운샘플링 레이어를 가지고 있기 때문입니다. 다음 코드를 실행하여 확인할 수 있습니다: \n",
    "\n",
    "2 ** (len(vae.config.block_out_channels) - 1) == 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47a6b3f-6720-4f0a-a246-47f0758e83fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=torch_device).manual_seed(123123)\n",
    "\n",
    "latents = torch.randn(\n",
    "    (batch_size, unet.config.in_channels, height // 8, width // 8),\n",
    "    generator=generator,\n",
    "    device=torch_device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac24bb5-7bae-4f87-b5d1-176195deaa70",
   "metadata": {},
   "source": [
    "**이미지 노이즈 제거**  \n",
    "먼저 초기 노이즈 분포인 *sigma*(노이즈 스케일 값)로 입력을 스케일링합니다. 이는 UniPCMultistepScheduler와 같은 개선된 스케줄러에 필요합니다:\n",
    "\n",
    "```python\n",
    "latents = latents * scheduler.init_noise_sigma\n",
    "```\n",
    "\n",
    "마지막 단계는 순수한 노이즈인 latents를 프롬프트에 설명된 이미지로 점진적으로 변환하는 디노이징 루프를 만드는 것입니다. 디노이징 루프는 다음 세 가지 작업을 수행해야 합니다:\n",
    "\n",
    "1. 디노이징 중 사용할 스케줄러의 timesteps를 설정합니다.\n",
    "2. timesteps를 반복합니다.\n",
    "3. 각 timestep에서 UNet 모델을 호출하여 노이즈 잔차를 예측하고, 이를 스케줄러에 전달하여 이전의 노이즈가 있는 샘플을 계산합니다.\n",
    "\n",
    "이 과정을 통해 점진적으로 노이즈가 제거되고 원하는 이미지가 생성됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3782cc5-78d4-4f5c-8d55-fe32d2743ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "for t in tqdm(scheduler.timesteps):\n",
    "    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "    latent_model_input = torch.cat([latents] * 2)\n",
    "\n",
    "    latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)\n",
    "\n",
    "    # predict the noise residual\n",
    "    with torch.no_grad():\n",
    "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "    # perform guidance\n",
    "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "    # compute the previous noisy sample x_t -> x_t-1\n",
    "    latents = scheduler.step(noise_pred, t, latents).prev_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c5d519-cb5e-4a7c-b4df-7bd8ded39c2a",
   "metadata": {},
   "source": [
    "**Decode the image**\n",
    "\n",
    "The final step is to use the vae to decode the latent representation into an image and get the decoded output with sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b04097f-2f16-490f-b224-d68d84b5f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale and decode the image latents with vae\n",
    "latents = 1 / 0.18215 * latents\n",
    "with torch.no_grad():\n",
    "    _image = vae.decode(latents).sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72fa3cb-29ab-4f57-93f8-dbb2a94bd0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image =(_image-_image.min()) / (_image.max() - _image.min())\n",
    "image = (image.permute(2,3,1,0) * 255).to(torch.uint8).cpu().numpy()\n",
    "image = image[:,:,:,0]\n",
    "image = Image.fromarray(image)\n",
    "image.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
